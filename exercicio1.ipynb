{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d20c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "from urllib.parse import urljoin\n",
    "import json\n",
    "\n",
    "base_url = \"https://pt.wikipedia.org\"\n",
    "paginas_visitadas = set()\n",
    "fila_paginas = [base_url]\n",
    "total_paginas = 0\n",
    "max_paginas = 5000\n",
    "diretorio_dados = \"paginas_wikipedia\"\n",
    "diretorio_infoboxes = \"infoboxes_wikipedia\"\n",
    "tempo_espera = 1\n",
    "\n",
    "if not os.path.exists(diretorio_dados):\n",
    "    os.makedirs(diretorio_dados)\n",
    "\n",
    "mapa_titulos = {}\n",
    "\n",
    "def normalizar_nome_arquivo(titulo):\n",
    "    caracteres_invalidos = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|', '\\n', '\\r', '\\t']\n",
    "    for char in caracteres_invalidos:\n",
    "        titulo = titulo.replace(char, ' ')\n",
    "    \n",
    "    while '  ' in titulo:\n",
    "        titulo = titulo.replace('  ', ' ')\n",
    "    \n",
    "    titulo = titulo.strip()\n",
    "    \n",
    "    if len(titulo) > 100:\n",
    "        titulo = titulo[:100]\n",
    "    \n",
    "    return titulo\n",
    "\n",
    "def obter_pagina(url):\n",
    "    try:\n",
    "        headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/91.0.4472.124'\n",
    "        }\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            return response.content\n",
    "        else:\n",
    "            print(f\"Erro ao acessar: {url}. Código: {response.status_code}\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao acessar: {url}. Erro: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def processar_links(soup):\n",
    "    links = []\n",
    "    \n",
    "    todos_links = soup.find_all(\"a\")\n",
    "    for link in todos_links:\n",
    "        if \"href\" in link.attrs.keys() and link[\"href\"].startswith(\"/wiki/\"):\n",
    "            if \":\" in link[\"href\"]:\n",
    "                continue\n",
    "            \n",
    "            url_completa = urljoin(base_url, link[\"href\"])\n",
    "            \n",
    "            if url_completa not in paginas_visitadas and url_completa not in fila_paginas:\n",
    "                links.append(url_completa)\n",
    "    \n",
    "    return links\n",
    "\n",
    "def salvar_pagina(conteudo, soup):\n",
    "    try:\n",
    "        titulo = None\n",
    "        \n",
    "        if soup.title:\n",
    "            titulo = soup.title.text.strip()\n",
    "            if \" – Wikipédia\" in titulo:\n",
    "                titulo = titulo.split(\" – Wikipédia\")[0].strip()\n",
    "            elif \" - Wikipédia\" in titulo:\n",
    "                titulo = titulo.split(\" - Wikipédia\")[0].strip()\n",
    "            \n",
    "        nome_arquivo = normalizar_nome_arquivo(titulo)\n",
    "        caminho_arquivo = os.path.join(diretorio_dados, f\"{nome_arquivo}.html\")\n",
    "        \n",
    "        with open(caminho_arquivo, \"wb\") as f:\n",
    "            f.write(conteudo)\n",
    "            \n",
    "        return nome_arquivo\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao salvar página: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "print(\"Iniciando coleta de páginas da Wikipedia...\")\n",
    "while fila_paginas and total_paginas < max_paginas:\n",
    "    url_atual = fila_paginas.pop(0)\n",
    "    \n",
    "    if url_atual in paginas_visitadas:\n",
    "        continue\n",
    "        \n",
    "    print(f\"Analisando página {total_paginas+1}/{max_paginas}: {url_atual}\")\n",
    "    \n",
    "    paginas_visitadas.add(url_atual)\n",
    "    \n",
    "    conteudo = obter_pagina(url_atual)\n",
    "    if not conteudo:\n",
    "        continue\n",
    "    \n",
    "    soup = BeautifulSoup(conteudo, \"html.parser\")\n",
    "    \n",
    "    nome_arquivo = salvar_pagina(conteudo, soup)\n",
    "    \n",
    "    novos_links = processar_links(soup)\n",
    "    \n",
    "    fila_paginas.extend(novos_links)\n",
    "\n",
    "    total_paginas += 1\n",
    "    \n",
    "    time.sleep(tempo_espera + random.random())\n",
    "\n",
    "print(f\"Coleta finalizada! Coletadas {total_paginas} páginas da Wikipedia.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72763b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(diretorio_dados):\n",
    "    print(f\"Erro: Pasta {diretorio_dados} não encontrada! Execute a Tarefa 1 primeiro.\")\n",
    "\n",
    "if not os.path.exists(diretorio_infoboxes):\n",
    "    os.makedirs(diretorio_infoboxes)\n",
    "\n",
    "def sanitizar_nome_arquivo(titulo):\n",
    "    caracteres_invalidos = ['\\\\', '/', ':', '*', '?', '\"', '<', '>', '|', '\\n', '\\r', '\\t']\n",
    "    for char in caracteres_invalidos:\n",
    "        titulo = titulo.replace(char, ' ')\n",
    "    \n",
    "    while '  ' in titulo:\n",
    "        titulo = titulo.replace('  ', ' ')\n",
    "    \n",
    "    titulo = titulo.strip()\n",
    "    \n",
    "    if len(titulo) > 100:\n",
    "        titulo = titulo[:100]\n",
    "    \n",
    "    return titulo\n",
    "\n",
    "def gerar_nome_arquivo_unico(titulo_base, diretorio):\n",
    "    nome_arquivo = f\"{titulo_base}.json\"\n",
    "    caminho_completo = os.path.join(diretorio, nome_arquivo)\n",
    "    \n",
    "    contador = 1\n",
    "    while os.path.exists(caminho_completo):\n",
    "        nome_arquivo = f\"{titulo_base}_{contador}.json\"\n",
    "        caminho_completo = os.path.join(diretorio, nome_arquivo)\n",
    "        contador += 1\n",
    "        \n",
    "    return nome_arquivo\n",
    "\n",
    "def verificar_infobox(soup):\n",
    "    for tabela in soup.find_all(\"table\"):\n",
    "        if not tabela.has_attr(\"class\"):\n",
    "            continue\n",
    "            \n",
    "        classes = tabela[\"class\"] if isinstance(tabela[\"class\"], list) else tabela[\"class\"].split()\n",
    "        \n",
    "        if \"infobox\" in classes:\n",
    "            return True, tabela\n",
    "        elif \"infobox_v2\" in classes:\n",
    "            return True, tabela\n",
    "    \n",
    "    return False, None\n",
    "\n",
    "def extrair_texto(elemento):\n",
    "    if not elemento:\n",
    "        return \"\"\n",
    "    \n",
    "    for tag in elemento.find_all([\"sup\", \"style\", \"script\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    texto = elemento.get_text(separator=\" \", strip=True)\n",
    "    \n",
    "    texto = texto.replace(\" .\", \".\").replace(\" ,\", \",\").replace(\" :\", \":\").replace(\" ;\", \";\")\n",
    "    texto = texto.replace(\" )\", \")\").replace(\"( \", \"(\").replace(\" ]\", \"]\").replace(\"[ \", \"[\")\n",
    "    \n",
    "    while \"  \" in texto:\n",
    "        texto = texto.replace(\"  \", \" \")\n",
    "    \n",
    "    return texto.strip()\n",
    "\n",
    "def extrair_pares_chave_valor(infobox):\n",
    "    dados_infobox = {}\n",
    "    \n",
    "    primeiro_th = infobox.find(\"th\")\n",
    "    \n",
    "    for linha in infobox.find_all(\"tr\"):\n",
    "        celula_chave = linha.find(\"td\", attrs={\"scope\": \"row\"})\n",
    "        if celula_chave:\n",
    "            chave = extrair_texto(celula_chave)\n",
    "            if not chave:\n",
    "                continue\n",
    "                \n",
    "            celulas_valor = linha.find_all(\"td\")\n",
    "            for i, td in enumerate(celulas_valor):\n",
    "                if td == celula_chave and i+1 < len(celulas_valor):\n",
    "                    valor = extrair_texto(celulas_valor[i+1])\n",
    "                    dados_infobox[chave] = valor\n",
    "                    break\n",
    "        \n",
    "        celula_chave = linha.find(\"th\", attrs={\"scope\": \"row\"})\n",
    "        if celula_chave:\n",
    "            chave = extrair_texto(celula_chave)\n",
    "            if not chave:\n",
    "                continue\n",
    "                \n",
    "            celulas_valor = linha.find_all(\"td\")\n",
    "            if celulas_valor:\n",
    "                valor = extrair_texto(celulas_valor[0])\n",
    "                dados_infobox[chave] = valor\n",
    "    \n",
    "    for linha in infobox.find_all(\"tr\"):\n",
    "        ths = linha.find_all(\"th\")\n",
    "        tds = linha.find_all(\"td\")\n",
    "        \n",
    "        if len(ths) == 1 and len(tds) == 1:\n",
    "            if ths[0] != primeiro_th:\n",
    "                chave = extrair_texto(ths[0])\n",
    "                valor = extrair_texto(tds[0])\n",
    "                \n",
    "                if chave and not ths[0].has_attr(\"colspan\"):\n",
    "                    dados_infobox[chave] = valor\n",
    "    \n",
    "    return dados_infobox\n",
    "\n",
    "def processar_pagina(caminho_arquivo):\n",
    "    try:\n",
    "        with open(caminho_arquivo, \"r\", encoding=\"utf-8\") as f:\n",
    "            conteudo = f.read()\n",
    "        \n",
    "        soup = BeautifulSoup(conteudo, \"html.parser\")\n",
    "        \n",
    "        tem_infobox, infobox = verificar_infobox(soup)\n",
    "        if not tem_infobox:\n",
    "            return None, None\n",
    "        \n",
    "        titulo = None\n",
    "        \n",
    "        primeiro_th = infobox.find(\"th\")\n",
    "        if primeiro_th:\n",
    "            span_no_th = primeiro_th.find(\"span\")\n",
    "            if span_no_th:\n",
    "                titulo = extrair_texto(span_no_th)\n",
    "            else:\n",
    "                titulo = extrair_texto(primeiro_th)\n",
    "        \n",
    "        dados_infobox = extrair_pares_chave_valor(infobox)\n",
    "        \n",
    "        return titulo, dados_infobox\n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar {caminho_arquivo}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "arquivos_html = [os.path.join(diretorio_dados, arquivo) \n",
    "                for arquivo in os.listdir(diretorio_dados) \n",
    "                if arquivo.endswith(\".html\")]\n",
    "\n",
    "total_arquivos = len(arquivos_html)\n",
    "arquivos_com_infobox = 0\n",
    "infoboxes_extraidas = 0\n",
    "\n",
    "for i, caminho_arquivo in enumerate(arquivos_html):\n",
    "    nome_arquivo = os.path.basename(caminho_arquivo)\n",
    "    \n",
    "    titulo, dados_infobox = processar_pagina(caminho_arquivo)\n",
    "    \n",
    "    if not titulo or not dados_infobox:\n",
    "        continue\n",
    "    \n",
    "    if len(dados_infobox) == 0:\n",
    "        continue\n",
    "    \n",
    "    arquivos_com_infobox += 1\n",
    "    \n",
    "    dados_finais = {\"__arquivo_origem\": nome_arquivo}\n",
    "    dados_finais.update(dados_infobox)\n",
    "    \n",
    "    titulo_limpo = sanitizar_nome_arquivo(titulo)\n",
    "    nome_json = gerar_nome_arquivo_unico(titulo_limpo, diretorio_infoboxes)\n",
    "    caminho_json = os.path.join(diretorio_infoboxes, nome_json)\n",
    "    \n",
    "    with open(caminho_json, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(dados_finais, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    infoboxes_extraidas += 1\n",
    "\n",
    "print(f\"\\nConcluído! Encontradas {arquivos_com_infobox} páginas com infobox.\")\n",
    "print(f\"Total de {infoboxes_extraidas} infoboxes extraídas com sucesso.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
